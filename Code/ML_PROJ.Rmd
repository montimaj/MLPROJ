---
title: "IST 5535 | Machine Learning Project: New York-Rent Prices-2020-Zillow Dataset"
author: "Group 2 | Tanner Fry, Sayantan Majumdar, Daren Liu"
date: "`r Sys.Date()`"
output: 
  html_document:
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
    echo = TRUE,
    message = FALSE,
    warning = FALSE,
    comment = NA)
```
  
```{r clean, echo=FALSE}
# Clean the environment
rm(list = ls())
options(digits = 3)
```

# Data Exploration

```{r data_explore}
zillow.data <- read.csv('../Data/zillow.csv')
str(zillow.data)
summary(zillow.data)
```

# Data Cleaning

```{r data_cleaning}
zillow.data <- subset(zillow.data, 
                      select = -c(yearBuilt: longitude,
                                  real.estate.provider:commute, latitude,
                                  city, state))
zillow.data <- na.omit(zillow.data)
# there are two rows in the na removed df where price is blank

transform_price <- function(price, sep = ',') {
  library(qdapRegex)
  
  if (price == '' || is.na(price)) {  # Fixes blank issue
    return(NA) 
  }
  price <- paste(price, '/', sep = '')
  check_plus <- ex_between(price, '$', '+')[[1]]
  check_bracket <- ex_between(price, '$', ')')[[1]]
  check_slash <- ex_between(price, '$', '/')[[1]]
  p <- NA
  if (!is.na(check_plus)) {
    p <- as.numeric(gsub(sep, "", check_plus))
  } else if (!is.na(check_bracket)) {
    p <- as.numeric(gsub(sep, "", check_bracket))
  } else if (!is.na(check_slash)) {
    p <- as.numeric(gsub(sep, "", check_slash))
  }
  return(p)
}

zillow.data$price <- sapply(zillow.data$price, transform_price)
hist(zillow.data$price, main = "Histogram of Actual Prices")
# handling outlier, changing the threshold will cause all models to perform poorly, also the dummies have to be adjusted according to this threshold.
zillow.data$price[zillow.data$price > 1e+6] <- NA
zillow.data <- na.omit(zillow.data)
str(zillow.data)
summary(zillow.data)
write.csv(zillow.data, '../Data/zillow_cleaned.csv', row.names = FALSE)
hist(zillow.data$price, main = "Histogram of Outlier Removed Prices")
```

# Data Exploration V2

```{r data_explore_v2, fig.height=7, fig.width=9}

psych::pairs.panels(zillow.data, 
             method = "pearson", # correlation method 
             hist.col = "green",
             density = TRUE,  # show density plots 
             ellipses = TRUE # show correlation ellipses
             )
```

Price of homes are positively related with the number of bedrooms and bathrooms (at least with most data points via bathrooms. Bathrooms might need to be refined as there seems to be a few outliers).

## Creating Dummies

```{r create_dummies}
zillow.data$homeStatusFOR_SALE <- ifelse(zillow.data$homeStatus ==
                                          'FOR_SALE', 1, 0)
zillow.data$homeStatusOTHER <- ifelse(zillow.data$homeStatus == 'OTHER', 
                                      1, 0)
zillow.data$homeStatusFOR_RENT <- ifelse(zillow.data$homeStatus ==
                                          'FOR_RENT', 1, 0)

zillow.data$homeTypeCONDO <- ifelse(zillow.data$homeType == 'CONDO', 1, 0)
zillow.data$homeTypeAPARTMENT <- ifelse(zillow.data$homeType ==
                                        'APARTMENT', 1, 0)
zillow.data$homeTypeMULTI_FAMILY <- ifelse(zillow.data$homeType ==
                                           'MULTI_FAMILY', 1, 0)
zillow.data$homeTypeSINGLE_FAMILY <- ifelse(zillow.data$homeType ==
                                           'SINGLE_FAMILY', 1, 0)
zillow.data$homeTypeTOWNHOUSE <- ifelse(zillow.data$homeType ==
                                           'TOWNHOUSE', 1, 0)

zillow.data$homeStatus <- NULL
zillow.data$homeType <- NULL
summary(zillow.data)
```

## Data Partition
```{r data_partition}
library(caret)
library(parallel)
library(doParallel)

# Required for parallelizing all caret function calls
cl <- makePSOCKcluster(detectCores())
registerDoParallel(cl)

set.seed(0)
# Doing a standard 80%-20% train-test split
zillow.train_index <- createDataPartition(zillow.data$price, p = .8,
                                          list = FALSE)
zillow.train_data <- zillow.data[zillow.train_index,]
zillow.test_data  <- zillow.data[-zillow.train_index,]
plot(zillow.train_data$price)
plot(zillow.test_data$price)
nrow(zillow.train_data)
nrow(zillow.test_data)
```

## Linear Model Selection and Regularization

### Removing Dummies to Resolve Multi-Collinearity

```{r remove_dummies}
zillow.train_data.linear <- zillow.train_data
zillow.test_data.linear <- zillow.test_data
zillow.train_data.linear$homeTypeTOWNHOUSE <- NULL
zillow.test_data.linear$homeTypeTOWNHOUSE <- NULL
zillow.train_data.linear$homeStatusFOR_SALE <- NULL
zillow.test_data.linear$homeStatusFOR_SALE <- NULL
```

### Univariate Feature Selection
```{r uni_feature_select}
zillow.linear.fit <- lm(price ~ ., data = zillow.train_data.linear)
summary(zillow.linear.fit)
print(BIC(zillow.linear.fit))
print(AIC(zillow.linear.fit))
```

### Best Subset Selection
```{r best_subset}
library(leaps)

train_x <- model.matrix(price ~ ., data = zillow.train_data.linear)[, -1]
train_y <- zillow.train_data.linear$price
nv_max <- NULL
fit_best <- regsubsets(x = train_x, y = train_y, nvmax = nv_max)
fit_best_sum <- summary(fit_best)

plot_model_stats <- function(fit_summary) {
  par(mfrow = c(2, 2))

  plot(fit_summary$rss,
       xlab = "Number of Variables",
       ylab = "RSS",
       type = "l")

  plot(fit_summary$adjr2,
       xlab = "Number of Variables",
       ylab = "Adjusted R2",
       type = "l")
  points(which.max(fit_summary$adjr2),
         fit_summary$adjr2[which.max(fit_summary$adjr2)],
         col = "red", cex = 2, pch = 20)
  plot(fit_summary$cp,
       xlab = " Number of Variables",
       ylab = " Cp",
       type = "l")
  points(which.min(fit_summary$cp),
         fit_summary$cp[which.min(fit_summary$cp)],
         col = "red", cex = 2, pch = 20)
  plot(fit_best_sum$bic,
       xlab = " Number of Variables",
       ylab = " BIC",
       type = "l")
  points(which.min(fit_summary$bic),
         fit_summary$bic[which.min(fit_summary$bic)],
         col = "red", cex = 2, pch = 20)
}
plot_model_stats(fit_best_sum)

model_stats <- function(fitted_model, num_var = 5, response = 'price') {
  coefs <- coef(fitted_model, num_var)
  print(coefs)
  var.names <- names(coefs)
  data.subset <- data.frame(train_x[, var.names[-1]])
  data.subset[[response]] <- train_y
  str(data.subset)
}
model_stats(fit_best)
```

### Forward Stepwise Selection
```{r forward_select}
fit_fwd <- regsubsets(x = train_x, y = train_y, nvmax = nv_max,
                      method = 'forward')
fit_fwd_sum <- summary(fit_fwd)
plot_model_stats(fit_fwd_sum)
model_stats(fit_fwd)
```


### Backward Stepwise Selection
```{r backward_select}
fit_bwd <- regsubsets(x = train_x, y = train_y, nvmax = nv_max,
                      method = 'backward')
fit_bwd_sum <- summary(fit_bwd)
plot_model_stats(fit_bwd_sum)
model_stats(fit_bwd)
```

### Lasso
```{r lasso}
library(glmnet)

grid <- 10 ^ seq(10, -2, length = 100)
lasso <- glmnet(x = train_x, y = train_y, alpha = 1, lambda = grid)
plot(lasso, xvar = 'lambda')
#k-folds CV
set.seed(0)
cv_out <- cv.glmnet(train_x, train_y, alpha = 1, lambda = grid,
                    nfolds = 10)
plot(cv_out)
best_lambda <- cv_out$lambda.min
print(best_lambda)
coefs <- predict(lasso, type = "coefficients", s = best_lambda)
print(coefs)
```

### Final Linear Model

```{r final_lm}
zillow.lfit <- lm(price ~ bathrooms + homeStatusOTHER +
                  homeStatusFOR_RENT + homeTypeCONDO + 
                  homeTypeMULTI_FAMILY, data = zillow.train_data.linear)
summary(zillow.lfit)
print(BIC(zillow.lfit))
print(AIC(zillow.lfit))
par(mfrow = c(2, 2))
plot(zillow.lfit)
par(mfrow = c(1, 1))
```

### Linear Model Evaluation
```{r lm_eval}
library(ggplot2)

zillow.test_pred <- predict(zillow.lfit, newdata = zillow.test_data.linear,
                    interval = "confidence")
eval_df <- cbind(zillow.test_data.linear, zillow.test_pred)
colnames(eval_df)[colnames(eval_df) == "price"] <- "Actual"
colnames(eval_df)[colnames(eval_df) == "fit"] <- "Predicted"
ggplot(eval_df, aes(x = Actual, y = Predicted)) +
    geom_point() +
    geom_line(aes(y = lwr), color = "red", linetype = "dashed")+
    geom_line(aes(y = upr), color = "red", linetype = "dashed")+
    geom_smooth(method = lm, se = TRUE)
postResample(eval_df$Predicted, eval_df$Actual)
plot(eval_df$Predicted, eval_df$Actual)
abline(0, 1, col = 'red')
```

## k-NN Regression

### Optimizing k
```{r knn_opt}
# For non-linear models, we directly use the original dummy containing data
preprocessParams <- preProcess(zillow.train_data,
                               method = c("scale", "center"))
train_scaled <- predict(preprocessParams, zillow.train_data)
test_scaled <- predict(preprocessParams, zillow.test_data)
control <- trainControl(method = 'repeatedcv',
                        number = 10, repeats = 3)
set.seed(0)
knn_tuned <- train(price ~ ., data = train_scaled, method = 'knn',
                   trControl = control,
                   tuneGrid = data.frame(k = 1: 100))
plot(knn_tuned)
```

### Final k-NN model
```{r final_knn}
price_knn <- train(price ~ ., data = train_scaled, method = 'knn',
                   trControl = trainControl(method = "none"),
                   tuneGrid = data.frame(k = 4))
price_knn
```

### k-NN Performance
```{r knn_perform}
knn_yhat <- predict(price_knn, newdata = test_scaled)
knn_yhat <- knn_yhat * sd(zillow.train_data$price) + 
           mean(zillow.train_data$price)
obs <- zillow.test_data$price
knn_results <- postResample(knn_yhat, obs)
print(knn_results)
plot(knn_yhat, obs)
abline(0, 1, col = 'red')
```

## Regression Tree

```{r rt}
library(tree)

price_rt <- tree(price ~ ., data = zillow.train_data)
plot(price_rt)
text(price_rt, cex = 0.5, col = 'red')
summary(price_rt)
```

### Pruned Regression Tree

```{r prt}
cv_price <- cv.tree(price_rt)
plot(cv_price$size, cv_price$dev, type = 'b', log = 'y',
     xlab = 'Tree Size', ylab = 'Deviance')
```

### Pruned Decision Tree Performance

```{r rt_perform}
rt_yhat <- predict(price_rt, newdata = zillow.test_data)
rt_results <- postResample(rt_yhat, obs)
print(rt_results)
plot(rt_yhat, obs)
abline(0, 1, col = 'red')
```

## Random Forest

```{r rf}
library(randomForest)
price_rf <- randomForest(price ~ ., data = zillow.train_data, mtry = 5,
                         importance = TRUE)
price_rf
```

### Optimizing mtry

```{r rf_opt}
num_predictors <- ncol(zillow.train_data) - 1
tuneGrid <- data.frame(mtry = 1: num_predictors)
control <- trainControl(method = 'repeatedcv',
                        number = 10, repeats = 3)
set.seed(0)
rf_tuned <- train(price ~ ., data = zillow.train_data,
                  method = 'rf',
                  trControl = control,
                  tuneGrid = tuneGrid)
print(rf_tuned)
plot(rf_tuned)
```

### Final RF model

```{r final_rf}
price_rf <- randomForest(price ~ ., data = zillow.train_data,
                         mtry = 10, importance = TRUE)
varImpPlot(price_rf)
```

### Random Forest Performance

```{r rf_perform}
rf_yhat <- predict(price_rf, newdata = zillow.test_data)
rf_results <- postResample(rf_yhat, obs)
print(rf_results)
plot(rf_yhat, obs)
abline(0, 1, col = 'red')
```

## Support Vector Machine

```{r svm}
svm_cv_caret <- function(data, response, num_folds, repeats, kernel_type) {
  set.seed(0)
  f <- as.formula(paste(response, '~ .'))
  tuneGrid <- data.frame(C = 10 ^ (-2: 2))
  if (kernel_type == 'svmRadial') {
    tuneGrid <- data.frame(C = 10 ^ (-2: 2),
                           sigma = 10 ^ (-2: 2))
  } else if (kernel_type == 'svmPoly') {
    tuneGrid <- data.frame(C = 10 ^ (-2: 2),
                           degree = rep(2, 5),
                           scale = rep(1, 5))
  }
  print(tuneGrid)
  control <- trainControl(method = 'repeatedcv',
                          number = num_folds,
                          repeats = repeats)
  svm_tuned <- train(f, data = data,
                     method = kernel_type,
                     trControl = control,
                     tuneGrid = tuneGrid)
  print(svm_tuned)
  print(svm_tuned$bestTune)
  plot(svm_tuned)
  return(svm_tuned)
}
```

### Linear Kernel

```{r tune_svm_linear}
tune_svm_linear <- svm_cv_caret(train_scaled, 'price', 10, 3,
                                'svmLinear')
```

### Radial (RBF) Kernel

```{r tune_svm_radial}
tune_svm_radial <- svm_cv_caret(train_scaled, 'price', 10, 3,
                                'svmRadial')
```

### Polynomial Kernel

```{r tune_svm_poly}
tune_svm_poly <- svm_cv_caret(train_scaled, 'price', 10, 3,
                              'svmPoly')
```

### Final SVM Model

```{r final_svm}
price_svm <- train(price ~ ., data = train_scaled,
                   method = 'svmRadial',
                   trControl = trainControl(method = "none"),
                   tuneGrid = data.frame(C = 10, sigma = 10))
```

### SVM Performance
```{r svm_perform}
svm_yhat <- predict(price_svm, newdata = test_scaled)
svm_yhat <- svm_yhat * sd(zillow.train_data$price) +
           mean(zillow.train_data$price)
svm_results <- postResample(svm_yhat, obs)
print(svm_results)
plot(svm_yhat, obs)
abline(0, 1, col = 'red')
```

## ANN

### ANN Preprocessing
```{r ann_preprocess}
preprocessParams <- preProcess(zillow.train_data, method = c("range"))
train_scaled <- predict(preprocessParams, zillow.train_data)
test_scaled <- predict(preprocessParams, zillow.test_data)
```

### ANN Fitting
```{r ann_fit_function}
library('neuralnet')
fit_ann <- function(data, response = 'price', hidden) {
  f <- formula(paste(response, '~ .'))
  nn_fit <- neuralnet(f, data = data, hidden = hidden,
                      linear.output = TRUE)
  return(nn_fit)
}
```

```{r ann_performance_func}
get_ann_performance <- function(nn_fit, test_scaled, train_scaled,
                                test_data, train_data,
                                response = 'price') {
  pred_norm_train <- compute(nn_fit, train_scaled)
  pred_norm_train <- pred_norm_train$net.result
  pred_train <- pred_norm_train*(max(train_data[[response]]) -
                       min(train_data[[response]])) +
                       min(train_data[[response]])
  pred_norm_test <- compute(nn_fit, test_scaled)
  pred_norm_test <- pred_norm_test$net.result
  pred_test <- pred_norm_test*(max(train_data[[response]]) -
                       min(train_data[[response]])) +
                       min(train_data[[response]])
  print('Training Metrics')
  print(postResample(pred_train, train_data[[response]]))
  print('Test Metrics')
  print(postResample(pred_test, test_data[[response]]))
  plot(pred_test, test_data[[response]])
  abline(0, 1, col = 'red')
}
```

## ANN Three-Layer Model
```{r nn_3_struct}
nn_fit_3 <- fit_ann(train_scaled, hidden = c(10, 5, 3))
summary(nn_fit_3)
plot(nn_fit_3, rep = "best", cex = 0.8)
```

## ANN Two-Layer Model
```{r nn_2_struct}
nn_fit_2 <- fit_ann(train_scaled, hidden = c(5, 3))
summary(nn_fit_2)
plot(nn_fit_2, rep = "best", cex = 0.8)
```

## ANN One-Layer Model
```{r nn_1_struct}
nn_fit_1 <- fit_ann(train_scaled, hidden = 5)
summary(nn_fit_1)
plot(nn_fit_1, rep = "best",cex = 0.8)
```

### ANN Three-Layer Performance
```{r compute_ann_3}
get_ann_performance(nn_fit_3, test_scaled, train_scaled,
                    zillow.test_data, zillow.train_data)
```

### ANN Two-Layer Performance
```{r compute_ann_2}
get_ann_performance(nn_fit_2, test_scaled, train_scaled,
                    zillow.test_data, zillow.train_data)
```

### ANN One-Layer Performance
```{r compute_ann1}
get_ann_performance(nn_fit_1, test_scaled, train_scaled,
                    zillow.test_data, zillow.train_data)
```

ANN One-Layer is performing best.

```{r stop_cluster}
stopCluster(cl)
```
